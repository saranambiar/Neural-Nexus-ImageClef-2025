{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365c983a",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e633c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch.autograd import Function\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import shutil\n",
    "import cv2\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1a22d1",
   "metadata": {},
   "source": [
    "# Setup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df881e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set()\n",
    "sns.set_palette('bwr')\n",
    "SNS_CMAP = 'bwr'\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.rcParams['grid.color'] = '#444444'\n",
    "colors = sns.palettes.color_palette(SNS_CMAP)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843e181",
   "metadata": {},
   "source": [
    "# Generated Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31481a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_generated = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # double checking that each image is of the same size\n",
    "    transforms.ToTensor(),         \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratedDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, subset=\"generated\"):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.subset = subset\n",
    "\n",
    "        self.folders = {\n",
    "            \"real_used\": os.path.join(root_dir, \"real_used\"),\n",
    "            \"real_not_used\": os.path.join(root_dir, \"real_not_used\"),\n",
    "            \"generated\": os.path.join(root_dir, \"generated\")\n",
    "        }\n",
    "\n",
    "        if subset == \"generated\":\n",
    "            folder = self.folders[\"generated\"]\n",
    "            for img_name in os.listdir(folder):\n",
    "                img_path = os.path.join(folder, img_name)\n",
    "                self.image_paths.append((img_path, \"generated\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # convert NumPy array to PIL Image for transformation\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48904d3",
   "metadata": {},
   "source": [
    "Note: Currently not including actual dataset in the repository to maintain privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eed5dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_generated = GeneratedDataset(root_dir= \"include dataset path here\", transform=transform_generated, subset=\"generated\")\n",
    "dataloader_generated = DataLoader(dataset_generated, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Total generated images: {len(dataset_generated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ddb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(dataset, num_images=10):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(20, 20))\n",
    "    for i in range(num_images):\n",
    "        image, label = dataset[i]\n",
    "\n",
    "        image = image.squeeze(0).numpy()  \n",
    "\n",
    "        image = (image - image.min()) / (image.max() - image.min())  # normalize for display\n",
    "\n",
    "        axes[i].imshow(image, cmap='gray')  \n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(f\"{label}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "show(dataset_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26019b40",
   "metadata": {},
   "source": [
    "# Real Used Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc11aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_real_used = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # double checking that each image is of the same size\n",
    "    transforms.ToTensor(),         \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealUsedDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, \"real_used\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564a9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_real_used = RealUsedDataset(root_dir= \"include dataset path here\", transform=transform_real_used, subset=\"real_used\")\n",
    "dataloader_real_used = DataLoader(dataset_real_used, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Total real used images: {len(dataset_real_used)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6ebfa",
   "metadata": {},
   "source": [
    "# Real Not Used Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6512a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_real_not_used = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # double checking that each image is of the same size\n",
    "    transforms.ToTensor(),         \n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbeb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNotUsedDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, \"real_not_used\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9298786",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_real_not_used = RealNotUsedDataset(root_dir= \"include dataset path here\", transform=transform_real_not_used, subset=\"real_not_used\")\n",
    "dataloader_real_not_used = DataLoader(dataset_real_not_used, batch_size=16, shuffle=True)\n",
    "\n",
    "print(f\"Total real not used images: {len(dataset_real_not_used)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
