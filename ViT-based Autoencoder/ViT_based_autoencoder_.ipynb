{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here, the encoder is pretrained on an external tuberculosis classification dataset, enabling it to\n",
        "extract domain-specific features. You can access this dataset here: https://www.kaggle.com/datasets/mohamedhanyyy/chest-ctscan-images"
      ],
      "metadata": {
        "id": "lfR5uY_Qnyid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import, load and preprocess external datset"
      ],
      "metadata": {
        "id": "0u2twEuRs3Uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   First load and augment the dataset.\n",
        "*   Then split it between training and validation datasets.\n",
        "*   Use the following code to train the transformer based encoder on the augmented external dataset"
      ],
      "metadata": {
        "id": "kmOHnjBWyyZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q graphviz torchsummary torchview"
      ],
      "metadata": {
        "id": "KtkujMo7tW0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "import torchvision\n",
        "from torchview import draw_graph\n",
        "import cv2\n",
        "import os\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from termcolor import colored\n",
        "import glob\n",
        "from typing import List, Tuple\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "qud5hPqutRY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "iONSAjTRtuYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "sns.set()\n",
        "sns.set_palette('bwr')\n",
        "SNS_CMAP = 'bwr'\n",
        "plt.style.use(\"dark_background\")\n",
        "plt.rcParams['grid.color'] = '#444444'\n",
        "colors = sns.palettes.color_palette(SNS_CMAP)\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "def clrd(text: str, color: str = None, con: bool = None, c1:str = 'ok', c2:str = 'error')->str:\n",
        "  text = str(text)\n",
        "    color_codes = {\n",
        "        'ok': '\\033[1;92m',\n",
        "        'error': '\\033[91m',\n",
        "        'warning': '\\033[93m',\n",
        "        'success': '\\033[92m',\n",
        "        'status': '\\033[95m',\n",
        "        'special': '\\033[94m',\n",
        "        'log': '\\033[96m',\n",
        "        'reset': '\\033[0m',\n",
        "    }\n",
        "    if con is not None:\n",
        "        color = c1 if con else c2\n",
        "    color_code = color_codes.get(color, color_codes['reset'])\n",
        "    return f\"{color_code}{text}{color_codes['reset']}\""
      ],
      "metadata": {
        "id": "Zio2RCrKs83s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = r\"/kaggle/input/chest-ctscan-images/Data\"\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
        "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
        "VAL_DIR = os.path.join(DATA_DIR, \"valid\")"
      ],
      "metadata": {
        "id": "Z00iW8iltel8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABEL_MAP = {\n",
        "    \"adenocarcinoma\": 0,\n",
        "    \"adenocarcinoma_left.lower.lobe_T2_N0_M0_Ib\": 0,\n",
        "    \"large.cell.carcinoma\": 1,\n",
        "    \"large.cell.carcinoma_left.hilum_T2_N2_M0_IIIa\": 1,\n",
        "    \"large.cell.carcinoma\": 1,\n",
        "    \"normal\": 2,\n",
        "    \"squamous.cell.carcinoma\": 3,\n",
        "    \"squamous.cell.carcinoma_left.hilum_T1_N2_M0_IIIa\": 3\n",
        "}\n",
        "\n",
        "train_paths, train_labels = [], []\n",
        "val_paths, val_labels = [], []\n",
        "\n",
        "for split in ['train', 'test', 'valid']:\n",
        "    split_path = os.path.join(DATA_DIR, split)\n",
        "    for folder in os.listdir(split_path):\n",
        "        full_path = os.path.join(split_path, folder)\n",
        "        if not os.path.isdir(full_path):\n",
        "            continue\n",
        "        label = LABEL_MAP.get(folder)\n",
        "        if label is None:\n",
        "            print(f\"Unknown folder label: {folder}\")\n",
        "            continue\n",
        "        for fname in os.listdir(full_path):\n",
        "            if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                img_path = os.path.join(full_path, fname)\n",
        "                if split == 'valid':\n",
        "                    val_paths.append(img_path)\n",
        "                    val_labels.append(label)\n",
        "                else:\n",
        "                    train_paths.append(img_path)\n",
        "                    train_labels.append(label)\n",
        "\n",
        "df_train = pd.DataFrame({\n",
        "    \"img\": train_paths,\n",
        "    \"label\": train_labels\n",
        "})\n",
        "df_valid = pd.DataFrame({\n",
        "    \"img\": val_paths,\n",
        "    \"label\": val_labels\n",
        "})\n",
        "df_train.shape, df_valid.shape"
      ],
      "metadata": {
        "id": "6-RLiwIAuexC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDatasetCSV(Dataset):\n",
        "    def __init__(self, df, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): `img` and `label` columns.\n",
        "            transform (callable, optional): Optional transforms to apply to the images.\n",
        "        \"\"\"\n",
        "        self.df = df.copy()\n",
        "        df['label'] = df['label'].apply(lambda y: torch.tensor(y, dtype=torch.long))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row['img']).convert(\"RGB\")\n",
        "        label = row['label']\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "IqYZixW4uiOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### augmentations on the dataset\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomAffine(\n",
        "        degrees=10,              # small rotation\n",
        "        translate=(0.05, 0.05),  # simulate slight offset\n",
        "        scale=(0.95, 1.05),      # simulate zoom in/out\n",
        "        shear=5                  # optional shear\n",
        "    ),\n",
        "    transforms.RandomApply([\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2)\n",
        "    ], p=0.8),\n",
        "    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "lq355SZyu2sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDatasetCSV(df_train, train_transforms)\n",
        "val_dataset = ImageDatasetCSV(df_valid, val_transforms)"
      ],
      "metadata": {
        "id": "1Y-znz65u7M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder training on external dataset"
      ],
      "metadata": {
        "id": "sbFK9PWYvKTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torchvision.models import vit_b_16\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=512):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.vit = vit_b_16(weights=None)\n",
        "        self.channel_adapter = nn.Conv2d(1, 3, kernel_size=1)\n",
        "        self.vit.heads = nn.Identity()  # removing classification head\n",
        "        self.fc = nn.Linear(768, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "        x = self.channel_adapter(x)\n",
        "        z = self.vit(x)\n",
        "        z = self.fc(z)\n",
        "        return z\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, encoder, latent_dim, num_classes):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.fc = nn.Linear(latent_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        out = self.fc(z)\n",
        "        return out"
      ],
      "metadata": {
        "id": "rphCdfANqdeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy_and_f1(y_true, y_pred):\n",
        "    preds = torch.argmax(y_pred, dim=1)\n",
        "    acc = accuracy_score(y_true.cpu(), preds.cpu())\n",
        "    f1 = f1_score(y_true.cpu(), preds.cpu(), average='macro')\n",
        "    return acc, f1\n",
        "\n",
        "def debug_grad_norm(module):\n",
        "    total_norm = 0\n",
        "    for p in module.parameters():\n",
        "        if p.grad is not None:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm.item() ** 2\n",
        "    total_norm = total_norm ** 0.5\n",
        "    return total_norm"
      ],
      "metadata": {
        "id": "XrWH-Gpsqfp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 4\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "num_gpu = torch.cuda.device_count()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "encoder = TransformerEncoder(latent_dim=512)\n",
        "model = TransformerClassifier(encoder=encoder, latent_dim=512, num_classes=NUM_CLASSES)\n",
        "model = model.to(device)\n",
        "\n",
        "if torch.cuda.device_count() > 1 and num_gpu > 1:\n",
        "    model = nn.DataParallel(model)\n",
        "    print(f\"Using {torch.cuda.device_count()} GPU(s)!\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9)"
      ],
      "metadata": {
        "id": "8P67YRTqqipN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "GW1gMZauqlFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from tqdm.notebook import tqdm\n",
        "except ImportError:\n",
        "    from tqdm import tqdm\n",
        "\n",
        "history = {\"train_acc\": [], \"train_loss\": [], \"val_acc\": [], \"val_loss\": []}\n",
        "\n",
        "epochs = 100\n",
        "DEBUG = False\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss, train_acc = 0.0, []\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "    for batch in pbar:\n",
        "        optimizer.zero_grad()\n",
        "        images, y = batch\n",
        "        images, y = images.to(device), y.to(device)\n",
        "        yhat = model(images)\n",
        "\n",
        "        loss = criterion(yhat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        acc, f1 = get_accuracy_and_f1(y, yhat)\n",
        "        train_loss += loss.item() / len(batch[0])\n",
        "        train_acc.append(acc)\n",
        "\n",
        "        pbar.set_postfix(loss=loss.item(), acc=acc)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    history[\"train_loss\"].append(train_loss)\n",
        "    history[\"train_acc\"].append(np.mean(train_acc))\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss, val_accs = 0.0, []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            images, y = batch\n",
        "            images, y = images.to(device), y.to(device)\n",
        "            yhat = model(images)\n",
        "\n",
        "            loss = criterion(yhat, y)\n",
        "            acc, f1 = get_accuracy_and_f1(y, yhat)\n",
        "\n",
        "            val_loss += loss.item() / len(batch[0])\n",
        "            val_accs.append(acc)\n",
        "\n",
        "    history[\"val_loss\"].append(val_loss)\n",
        "    history[\"val_acc\"].append(np.mean(val_accs))\n",
        "\n",
        "    # gradient norm of encoder\n",
        "    classifier_grad_norm = debug_grad_norm(model.module.encoder if isinstance(model, nn.DataParallel) else model.encoder)\n",
        "\n",
        "    print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Train Acc: {np.mean(train_acc):.4f} | Val Acc: {np.mean(val_accs):.4f} | Grad Norm: {classifier_grad_norm:.4f}\")\n",
        "\n",
        "    if DEBUG:\n",
        "        break"
      ],
      "metadata": {
        "id": "ihu8e9OIqoEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the encoder and use it in the following code."
      ],
      "metadata": {
        "id": "OVPpcPqnqtbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the autoencoder"
      ],
      "metadata": {
        "id": "MNWB2VmvvbUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the dataset given in the task\n",
        "\n",
        "Currently havent included the data here to maintain privacy"
      ],
      "metadata": {
        "id": "e9qWhgqAxdt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MedicalImageDatasetBlackWhite(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if fname.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "image_size = 256\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = MedicalImageDatasetBlackWhite(root_dir='insert dataset here',\n",
        "                              transform=transforms.Compose([\n",
        "                               transforms.Grayscale(),\n",
        "                               transforms.Resize((image_size, image_size)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(mean=[0.5], std=[0.5]),\n",
        "                           ]))"
      ],
      "metadata": {
        "id": "b-UAEnBXvlm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoder structure"
      ],
      "metadata": {
        "id": "vaH2QsnRwjKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualDecoderBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
        "        self.relu = nn.LeakyReLU(0.1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.skip(x)\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += identity\n",
        "        return self.relu(out)\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=512, out_channels=1):\n",
        "        super().__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.fc = nn.Linear(latent_dim, 256 * 8 * 8)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.decode = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 256, kernel_size=3, stride=2, padding=1, output_padding=1),  # 16x16\n",
        "            ResidualDecoderBlock(256, 128),\n",
        "            ResidualDecoderBlock(128, 64),\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # 32x32\n",
        "            ResidualDecoderBlock(64, 32),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),  # 64x64\n",
        "            ResidualDecoderBlock(16, 16),\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),    # 128x128\n",
        "            nn.ConvTranspose2d(8, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),  # 256x256\n",
        "            nn.Tanh()  # Assuming input images are normalized between [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        x = self.relu(self.fc(z))\n",
        "        x = x.view(-1, 256, 8, 8)\n",
        "        return self.decode(x)"
      ],
      "metadata": {
        "id": "ojrEOzp6wA0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(nn.Module):\n",
        "    def __init__(self, encoder: nn.Module, decoder: nn.Module):\n",
        "        super(AutoEncoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        recon = self.decoder(z)\n",
        "        return recon"
      ],
      "metadata": {
        "id": "Hl8y_nGprAnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "encoder = TransformerEncoder(latent_dim=512).to(device)\n",
        "decoder = TransformerDecoder(latent_dim=512, out_channels=1).to(device)\n",
        "autoencoder = AutoEncoder(encoder, decoder).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    autoencoder.train()\n",
        "    running_loss = 0.0\n",
        "    for images, _ in train_loader:  # Assuming train_loader yields (image, label) pairs\n",
        "        images = images.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = autoencoder(images)\n",
        "        loss = criterion(outputs, images)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader):.4f}\")"
      ],
      "metadata": {
        "id": "vXwF1K0MrD0v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}