{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd314b1",
   "metadata": {},
   "source": [
    "A DCLGAN is a Dual Contrastive Learning Generative Adversarial Network that is built on a generator and discriminator, and has a PatchNCE loss at its core that learns to generate synthetic images by dividing the dataset images into patches, treating them as positive and negative pairs - where positive pairs are pushed together in a vector space while negative pairs are pushed apart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e9a19",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed37d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms, models, utils\n",
    "from torchvision.models import inception_v3\n",
    "from torchvision.datasets import ImageFolder, DatasetFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torch.autograd import Function\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import shutil\n",
    "import cv2\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dacd77f",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6d509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0),\n",
    "            nn.InstanceNorm2d(dim),\n",
    "            nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0),\n",
    "            nn.InstanceNorm2d(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=1, output_nc=1, n_blocks=6):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initial convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, padding=0),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "\n",
    "        # Downsampling\n",
    "        n_downsampling = 2\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**i\n",
    "            model += [\n",
    "                nn.Conv2d(64 * mult, 64 * mult * 2, kernel_size=3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(64 * mult * 2),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "\n",
    "        # ResNet blocks\n",
    "        mult = 2**n_downsampling\n",
    "        for i in range(n_blocks):\n",
    "            model += [ResnetBlock(64 * mult)]\n",
    "\n",
    "        # Upsampling\n",
    "        for i in range(n_downsampling):\n",
    "            mult = 2**(n_downsampling - i)\n",
    "            model += [\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False),\n",
    "                nn.Conv2d(64 * mult, int(64 * mult / 2), kernel_size=3, stride=1, padding=1),\n",
    "                nn.InstanceNorm2d(int(64 * mult / 2)),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "\n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(64, output_nc, kernel_size=7, padding=0),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure output has same spatial dimensions as input\n",
    "        out = self.model(x)\n",
    "        if out.shape[2:] != x.shape[2:]:\n",
    "            out = F.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670a711a",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b1ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_nc=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)  # output as a feature map [batch_size, 1, H, W]\n",
    "        ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c890f",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9afc4d5",
   "metadata": {},
   "source": [
    "PatchNCE Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ff2a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchNCELoss(nn.Module):\n",
    "    \"\"\"Enhanced PatchNCE Loss with feature normalization\"\"\"\n",
    "    def __init__(self, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, feat_q, feat_k):\n",
    "        if feat_q.shape[2:] != feat_k.shape[2:]:\n",
    "            feat_q = F.interpolate(feat_q, size=feat_k.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        batch_size = feat_q.shape[0]\n",
    "        dim = feat_q.shape[1]\n",
    "\n",
    "        feat_q = feat_q.view(batch_size, dim, -1)\n",
    "        feat_k = feat_k.view(batch_size, dim, -1)\n",
    "\n",
    "        feat_q = F.normalize(feat_q, dim=1)\n",
    "        feat_k = F.normalize(feat_k, dim=1)\n",
    "\n",
    "        num_patches = feat_q.shape[2]\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(batch_size):\n",
    "            q = feat_q[i].permute(1, 0)\n",
    "            k = feat_k[i].permute(1, 0)\n",
    "\n",
    "            # Positive logits: num_patches x 1\n",
    "            l_pos = torch.bmm(q.view(num_patches, 1, dim),\n",
    "                             k.view(num_patches, dim, 1)).view(num_patches, 1)\n",
    "\n",
    "            # Negative logits: num_patches x (num_patches-1)\n",
    "            l_neg = torch.mm(q, k.t())\n",
    "\n",
    "            # self-similarity\n",
    "            identity_mask = torch.eye(num_patches, device=l_neg.device)\n",
    "            l_neg = l_neg.masked_fill(identity_mask.bool(), -float('inf'))\n",
    "\n",
    "            # combine positive and negative logits\n",
    "            logits = torch.cat([l_pos, l_neg], dim=1) / self.temperature\n",
    "\n",
    "            # positive pair is the first entry\n",
    "            labels = torch.zeros(num_patches, dtype=torch.long, device=logits.device)\n",
    "\n",
    "            loss += self.cross_entropy_loss(logits, labels)\n",
    "\n",
    "        return loss / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76ea40",
   "metadata": {},
   "source": [
    "Feature Matching Loss (to stabilize GAN training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ea5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureMatchingLoss(nn.Module):\n",
    "    \"\"\"Feature Matching Loss for stabilizing GAN training\"\"\"\n",
    "    def __init__(self):\n",
    "        super(FeatureMatchingLoss, self).__init__()\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "\n",
    "    def forward(self, real_features, fake_features):\n",
    "        loss = 0\n",
    "        for real_feat, fake_feat in zip(real_features, fake_features):\n",
    "            loss += self.l1_loss(fake_feat, real_feat.detach())\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f15b06",
   "metadata": {},
   "source": [
    "# Hounsfield Units Loss (New Introduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da7704f",
   "metadata": {},
   "source": [
    "Hounsfield Units (HU) are a standardized scale used in CT imaging that quantifies radiodensity. Different tissues have characteristic HU ranges:\n",
    "\n",
    "- Air: approximately -1000 HU\n",
    "- Lung tissue: -700 to -600 HU\n",
    "- Fat: -100 to -50 HU\n",
    "- Water: 0 HU\n",
    "- Soft tissue: +20 to +70 HU\n",
    "- Bone: +700 to +3000 HU\n",
    "\n",
    "This quantitative nature of CT scans makes them different from regular photographs - specific pixel intensity values have medical meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2268c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HULoss(nn.Module):\n",
    "    \"\"\"Hounsfield Unit distribution preservation loss for CT scans\"\"\"\n",
    "    def __init__(self, bins=100, min_value=-1, max_value=1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.bins = bins\n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, real, fake):\n",
    "        losses = []\n",
    "\n",
    "        # process each image in the batch individually for better histogram matching\n",
    "        for i in range(real.size(0)):\n",
    "            # calculate histograms of pixel values\n",
    "            real_hist = torch.histc(real[i].flatten(), bins=self.bins,\n",
    "                                  min=self.min_value, max=self.max_value)\n",
    "            fake_hist = torch.histc(fake[i].flatten(), bins=self.bins,\n",
    "                                  min=self.min_value, max=self.max_value)\n",
    "\n",
    "            # normalize histograms to make them probability distributions\n",
    "            real_hist = real_hist / (real_hist.sum() + 1e-10)\n",
    "            fake_hist = fake_hist / (fake_hist.sum() + 1e-10)\n",
    "\n",
    "            # KL divergence for distribution matching\n",
    "            # Adding small epsilon to avoid log(0)\n",
    "            eps = 1e-10\n",
    "            kl_div = (real_hist * torch.log((real_hist + eps) / (fake_hist + eps))).sum()\n",
    "            losses.append(kl_div)\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.stack(losses).mean()\n",
    "        else:\n",
    "            return torch.stack(losses).sum()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
